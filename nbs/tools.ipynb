{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1645104e",
   "metadata": {},
   "source": [
    "# Buliding Flexible Code Base\n",
    "\n",
    "We will build a flexible code base that increases the ease of development and code-reuse. I will take bits and pieces of FASTAI library and use this as a base for our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8480e3",
   "metadata": {},
   "source": [
    "## Let's setup dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd87c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "noise = 0.1\n",
    "X_train, y_train = datasets.make_moons(n_samples=2000, noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f1e79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4caf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn import Sequential\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "class DummyDset(Dataset):\n",
    "    def __init__(self,sz,noise=0.1):\n",
    "        self.x,self.y = datasets.make_moons(n_samples=sz,noise=noise)\n",
    "        self.x = self.x.astype('float32')\n",
    "    def __getitem__(self,ind):\n",
    "        return self.x[ind],self.y[ind]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "        \n",
    "        \n",
    "class Dloaders:\n",
    "    def __init__(self,*dls):\n",
    "        self.train,self.valid = dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7970b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampl = DummyDset(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5385cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30134398 -0.0563121 ],(2,)\n",
      "1,()\n"
     ]
    }
   ],
   "source": [
    "for x,y in sampl:\n",
    "    print(f'{x},{x.shape}')\n",
    "    print(f'{y},{y.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c145be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dset,vld_dset,tst_dset = DummyDset(2000),DummyDset(500),DummyDset(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b810d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_dset,shuffle=True,batch_size=50)\n",
    "vld_dl = DataLoader(vld_dset,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba425e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = Dloaders(trn_dl,vld_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4340386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "for x_bl,y_bl in trn_dl:\n",
    "    print(x_bl.shape)\n",
    "    print(y_bl.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb2b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = Sequential(nn.Linear(2,164),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Linear(164,32),\n",
    "          nn.LeakyReLU(),\n",
    "          nn.Linear(32,2),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719fca00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=164, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Linear(in_features=164, out_features=32, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01)\n",
       "  (4): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09816983",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_fn = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2780e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Python user-defined exceptions\n",
    "class CancelFitException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "56f46120",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = None,None\n",
    "for xb,yb in trn_dl:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "689d2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dmodel(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f19d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d54fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42ea4378",
   "metadata": {},
   "source": [
    "## Flexible Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05a77f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,model,dls,loss_func,lr,opt_func,cbs,device='cpu'):\n",
    "        self.model = model\n",
    "        self.dl = dls\n",
    "        self.loss_func = loss_func\n",
    "        self.lr = lr\n",
    "        self.opt_func = opt_func\n",
    "        self.cbs =cbs\n",
    "        self.device = device\n",
    "        for cb in cbs:cb.trainer = self\n",
    "        \n",
    "    def one_batch(self,batch):\n",
    "        self('before_batch')\n",
    "        xb,yb = batch\n",
    "        self.opt.zero_grad() \n",
    "        #storing y_preds onto the class for a potential use by a callback\n",
    "        self.y_preds = self.model(xb)\n",
    "        self.loss = self.loss_func(self.y_preds,yb)\n",
    "        if self.model.training:\n",
    "            #take backward step only if in training mode\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "        self('after_batch')\n",
    "        \n",
    "    def one_epoch(self,is_train):\n",
    "        self.model.training = is_train\n",
    "        self('before_epoch')\n",
    "        #choose the appropriate data-loader\n",
    "        dl = self.dl.train if is_train else self.dl.valid\n",
    "        for self.epochnum,self.batch in enumerate(dl):\n",
    "            #import pdb;pdb.set_trace()\n",
    "            self.one_batch(self.batch)\n",
    "        self('after_epoch')\n",
    "    \n",
    "    def fit(self,nepochs):\n",
    "        self('before_fit')\n",
    "        self.opt = self.opt_func(self.model.parameters(),self.lr)\n",
    "        self.nepochs = nepochs\n",
    "        try:\n",
    "            for self.epoch in tqdm(range(self.nepochs)):\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "        except CancelFitException:\n",
    "            pass\n",
    "        self('after_fit')\n",
    "        \n",
    "    def predict(self,dl):\n",
    "        print('NOT IMPLEMENTED')\n",
    "        pass\n",
    "    \n",
    "    def infer(self,x):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,name):\n",
    "        for cb in self.cbs:getattr(cb,name)()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89a02eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallBack:\n",
    "    def before_epoch(self):\n",
    "        pass\n",
    "    def after_epoch(self):\n",
    "        pass\n",
    "    def before_batch(self):\n",
    "        pass\n",
    "    def after_batch(self):\n",
    "        pass\n",
    "    def before_fit(self):\n",
    "        pass\n",
    "    def after_fit(self): \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class SetupTrainerCB(CallBack):\n",
    "    def before_batch(self):\n",
    "        xb,yb = to_device(self.trainer.device,self.trainer.batch)\n",
    "        self.trainer.batch = xb,yb\n",
    "\n",
    "    def before_fit(self): \n",
    "        if self.trainer.device != 'cpu':\n",
    "            self.trainer.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fa78d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrinterCB(CallBack):\n",
    "    def before_epoch(self):\n",
    "        self.ns,self.losses,self.accs = [],[],[]\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        n = sum(self.ns)\n",
    "        epoch_type = 'Training' if self.trainer.model.training else 'Validation'\n",
    "        acc = sum(self.accs)/n\n",
    "        loss = sum(self.losses)/n\n",
    "        print(f\"{epoch_type} acc: {acc},loss:{loss}\")\n",
    "        \n",
    "    def after_batch(self):\n",
    "        xb,yb = self.trainer.batch\n",
    "        acc = (self.trainer.y_preds.argmax(dim=-1) == yb).float().sum()\n",
    "        ns = len(xb)\n",
    "        l = (self.trainer.loss.item())*ns\n",
    "        self.ns.append(ns)\n",
    "        self.losses.append(l)\n",
    "        self.accs.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "460fbf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [PrinterCB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63b9cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(dmodel,dls,loss_fn,0.0002,optimizer_fn,cbs,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b0b9183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 41.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.6265000104904175,loss:0.6529418170452118\n",
      "Validation acc: 0.777999997138977,loss:0.5868911445140839\n",
      "Training acc: 0.8105000257492065,loss:0.5206522881984711\n",
      "Validation acc: 0.8119999766349792,loss:0.4602777689695358\n",
      "Training acc: 0.8289999961853027,loss:0.40951949581503866\n",
      "Validation acc: 0.8299999833106995,loss:0.3736293613910675\n",
      "Training acc: 0.8510000109672546,loss:0.3374750755727291\n",
      "Validation acc: 0.8539999723434448,loss:0.3180138051509857\n",
      "Training acc: 0.8734999895095825,loss:0.289821594581008\n",
      "Validation acc: 0.8700000047683716,loss:0.2798048108816147\n",
      "Training acc: 0.887499988079071,loss:0.25773362182080745\n",
      "Validation acc: 0.8920000195503235,loss:0.2542142793536186\n",
      "Training acc: 0.8999999761581421,loss:0.23499636426568032\n",
      "Validation acc: 0.8980000019073486,loss:0.23408746123313903\n",
      "Training acc: 0.9079999923706055,loss:0.21637702491134406\n",
      "Validation acc: 0.8960000276565552,loss:0.21646395623683928\n",
      "Training acc: 0.9144999980926514,loss:0.1996240969747305\n",
      "Validation acc: 0.9020000100135803,loss:0.20010672360658646\n",
      "Training acc: 0.9185000061988831,loss:0.18470573723316192\n",
      "Validation acc: 0.9079999923706055,loss:0.18504095077514648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009d2d6",
   "metadata": {},
   "source": [
    "## Using tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4d09739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cwd = os.getcwd()\n",
    "module_path = \"/\".join(cwd.split('/')[0:-1])\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9b79249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import vgg16\n",
    "\n",
    "\n",
    "def accuracy(y_hat,y):\n",
    "    #import pdb;pdb.set_trace()\n",
    "    return torch.mean((y_hat == y).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dcfbf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring pytorch-lightning with wandb\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0020ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_model = {'dmodel':dmodel}\n",
    "\n",
    "hparams = {'model':'dmodel','optimizer':{'lr':0.0002}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "601e26fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.model = str_model[hparams['model']]\n",
    "        #import pdb;pdb.set_trace()\n",
    "    def forward(self,x):\n",
    "        #? write for reading off any model specific hparams before the call(hydra)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_hat = self.model(x)\n",
    "        #y_true = y.squeeze().type(torch.LongTensor)\n",
    "        loss = F.cross_entropy(y_hat,y)\n",
    "        \n",
    "        acc = torch.mean((y_hat.argmax(dim=-1) == y).type(torch.float32))\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #print(f'batch indx:{batch_idx},{loss.item()}')\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss,on_step=False, on_epoch=True)\n",
    "        \n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        #y_hat = torch.argmax(y_hat, dim=1)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        acc = torch.mean((y_hat.argmax(dim=-1) == y).type(torch.float32))\n",
    "        self.log('val_acc', acc,on_step=False, on_epoch=True)\n",
    "        return acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams['optimizer']['lr'])\n",
    "        return optimizer\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f6447f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pl.seed_everything(0)\n",
    "#model\n",
    "net = Model(hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "092e5f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 5.8 K \n",
      "-------------------------------------\n",
      "5.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/mambaforge/envs/uncertainty-estimates/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 0\n",
      "/Users/vinay/mambaforge/envs/uncertainty-estimates/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/vinay/mambaforge/envs/uncertainty-estimates/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:412: UserWarning: The number of training samples (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|█████████▌  | 40/50 [00:00<00:00, 404.19it/s, loss=0.639, v_num=4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|████████████| 50/50 [00:00<00:00, 449.71it/s, loss=0.639, v_num=4]\u001b[A\n",
      "Epoch 1:  80%|█████████▌  | 40/50 [00:00<00:00, 468.66it/s, loss=0.525, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|████████████| 50/50 [00:00<00:00, 528.17it/s, loss=0.525, v_num=4]\u001b[A\n",
      "Epoch 2:  80%|███████████▏  | 40/50 [00:00<00:00, 489.90it/s, loss=0.4, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████████| 50/50 [00:00<00:00, 548.65it/s, loss=0.4, v_num=4]\u001b[A\n",
      "Epoch 3:  80%|█████████▌  | 40/50 [00:00<00:00, 495.48it/s, loss=0.321, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|████████████| 50/50 [00:00<00:00, 554.34it/s, loss=0.321, v_num=4]\u001b[A\n",
      "Epoch 4:  80%|█████████▌  | 40/50 [00:00<00:00, 495.85it/s, loss=0.264, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|████████████| 50/50 [00:00<00:00, 551.60it/s, loss=0.264, v_num=4]\u001b[A\n",
      "Epoch 5:  80%|█████████▌  | 40/50 [00:00<00:00, 492.89it/s, loss=0.225, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|████████████| 50/50 [00:00<00:00, 552.51it/s, loss=0.225, v_num=4]\u001b[A\n",
      "Epoch 6:  80%|██████████▍  | 40/50 [00:00<00:00, 491.23it/s, loss=0.19, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████████| 50/50 [00:00<00:00, 550.43it/s, loss=0.19, v_num=4]\u001b[A\n",
      "Epoch 7:  80%|█████████▌  | 40/50 [00:00<00:00, 485.77it/s, loss=0.172, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|████████████| 50/50 [00:00<00:00, 543.12it/s, loss=0.172, v_num=4]\u001b[A\n",
      "Epoch 8:  80%|█████████▌  | 40/50 [00:00<00:00, 502.20it/s, loss=0.152, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|████████████| 50/50 [00:00<00:00, 562.19it/s, loss=0.152, v_num=4]\u001b[A\n",
      "Epoch 9:  80%|█████████▌  | 40/50 [00:00<00:00, 489.35it/s, loss=0.135, v_num=4]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|████████████| 50/50 [00:00<00:00, 546.01it/s, loss=0.135, v_num=4]\u001b[A\n",
      "Epoch 9: 100%|████████████| 50/50 [00:00<00:00, 511.66it/s, loss=0.135, v_num=4]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "logger = CSVLogger(\"logs\", name=\"temp\")\n",
    "trainer = pl.Trainer(max_epochs=10,logger=logger)\n",
    "trainer.fit(net,trn_dl,vld_dl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c969f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6b8d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/Users/vinay/Projects/uncertainty-estimates/nbs/logs/temp/version_4/'\n",
    "met = pd.read_csv(log_dir+'metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a326ea50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_acc</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.667147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.840</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>0.6930</td>\n",
       "      <td>0.560631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.862</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.429429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.884</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.328797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.900</td>\n",
       "      <td>4</td>\n",
       "      <td>199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>199</td>\n",
       "      <td>0.8900</td>\n",
       "      <td>0.266220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.916</td>\n",
       "      <td>5</td>\n",
       "      <td>239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>239</td>\n",
       "      <td>0.9015</td>\n",
       "      <td>0.228299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.928</td>\n",
       "      <td>6</td>\n",
       "      <td>279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>279</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.202206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.930</td>\n",
       "      <td>7</td>\n",
       "      <td>319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>319</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>0.180768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.936</td>\n",
       "      <td>8</td>\n",
       "      <td>359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>359</td>\n",
       "      <td>0.9365</td>\n",
       "      <td>0.161462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.938</td>\n",
       "      <td>9</td>\n",
       "      <td>399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>399</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.143971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_acc  epoch  step  train_acc  train_loss\n",
       "0     0.500      0    39        NaN         NaN\n",
       "1       NaN      0    39     0.5000    0.667147\n",
       "2     0.840      1    79        NaN         NaN\n",
       "3       NaN      1    79     0.6930    0.560631\n",
       "4     0.862      2   119        NaN         NaN\n",
       "5       NaN      2   119     0.8540    0.429429\n",
       "6     0.884      3   159        NaN         NaN\n",
       "7       NaN      3   159     0.8735    0.328797\n",
       "8     0.900      4   199        NaN         NaN\n",
       "9       NaN      4   199     0.8900    0.266220\n",
       "10    0.916      5   239        NaN         NaN\n",
       "11      NaN      5   239     0.9015    0.228299\n",
       "12    0.928      6   279        NaN         NaN\n",
       "13      NaN      6   279     0.9150    0.202206\n",
       "14    0.930      7   319        NaN         NaN\n",
       "15      NaN      7   319     0.9275    0.180768\n",
       "16    0.936      8   359        NaN         NaN\n",
       "17      NaN      8   359     0.9365    0.161462\n",
       "18    0.938      9   399        NaN         NaN\n",
       "19      NaN      9   399     0.9455    0.143971"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc35b26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d604080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.model = str_model[hparams['model']]\n",
    "        #import pdb;pdb.set_trace()\n",
    "    def forward(self,x):\n",
    "        #? write for reading off any model specific hparams before the call(hydra)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_hat = self.model(x)\n",
    "        #y_true = y.squeeze().type(torch.LongTensor)\n",
    "        loss = F.cross_entropy(y_hat,y)\n",
    "        \n",
    "        acc = torch.mean((y_hat.argmax(dim=-1) == y).type(torch.float32))\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #print(f'batch indx:{batch_idx},{loss.item()}')\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        #y_hat = torch.argmax(y_hat, dim=1)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        acc = torch.mean((y_hat.argmax(dim=-1) == y).type(torch.float32))\n",
    "        self.log('val_acc', acc,on_step=False, on_epoch=True)\n",
    "        return acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams['optimizer']['lr'])\n",
    "        return optimizer\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f0f093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/mambaforge/envs/uncertainty-estimates/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvin136\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating wandb\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb.login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c1aefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project=\"temp-tools\")\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "csvlogger = CSVLogger(\"logs\", name=\"temp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eca57d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pl.seed_everything(0)\n",
    "#model\n",
    "net = Model(hparams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=[wandb_logger,csvlogger],    # W&B integration\n",
    "    log_every_n_steps=50,   # set the logging frequency                \n",
    "    max_epochs=10,           # number of epochs\n",
    "    deterministic=True,     # keep it deterministic\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e48a7b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vin136/temp-tools/runs/2n0cca8l\" target=\"_blank\">wild-grass-1</a></strong> to <a href=\"https://wandb.ai/vin136/temp-tools\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 5.8 K \n",
      "-------------------------------------\n",
      "5.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/mambaforge/envs/uncertainty-estimates/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 0\n",
      "/Users/vinay/mambaforge/envs/uncertainty-estimates/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/vinay/mambaforge/envs/uncertainty-estimates/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:412: UserWarning: The number of training samples (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|███████▏ | 40/50 [00:00<00:00, 331.27it/s, loss=0.581, v_num=8l_5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████| 50/50 [00:00<00:00, 373.13it/s, loss=0.581, v_num=8l_5]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 40/50 [00:00<00:00, 403.25it/s, loss=0.453, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████| 50/50 [00:00<00:00, 456.06it/s, loss=0.453, v_num=8l_5]\u001b[A\n",
      "Epoch 2:  80%|███████▏ | 40/50 [00:00<00:00, 296.53it/s, loss=0.358, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████| 50/50 [00:00<00:00, 343.49it/s, loss=0.358, v_num=8l_5]\u001b[A\n",
      "Epoch 3:  80%|███████▏ | 40/50 [00:00<00:00, 358.06it/s, loss=0.283, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████| 50/50 [00:00<00:00, 399.74it/s, loss=0.283, v_num=8l_5]\u001b[A\n",
      "Epoch 4:  80%|███████▏ | 40/50 [00:00<00:00, 445.32it/s, loss=0.262, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████| 50/50 [00:00<00:00, 485.54it/s, loss=0.262, v_num=8l_5]\u001b[A\n",
      "Epoch 5:  80%|███████▏ | 40/50 [00:00<00:00, 464.28it/s, loss=0.214, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████| 50/50 [00:00<00:00, 520.21it/s, loss=0.214, v_num=8l_5]\u001b[A\n",
      "Epoch 6:  80%|███████▏ | 40/50 [00:00<00:00, 410.86it/s, loss=0.187, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████| 50/50 [00:00<00:00, 450.01it/s, loss=0.187, v_num=8l_5]\u001b[A\n",
      "Epoch 7:  80%|███████▏ | 40/50 [00:00<00:00, 301.42it/s, loss=0.159, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████| 50/50 [00:00<00:00, 346.17it/s, loss=0.159, v_num=8l_5]\u001b[A\n",
      "Epoch 8:  80%|███████▏ | 40/50 [00:00<00:00, 407.58it/s, loss=0.163, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████| 50/50 [00:00<00:00, 449.08it/s, loss=0.163, v_num=8l_5]\u001b[A\n",
      "Epoch 9:  80%|███████▏ | 40/50 [00:00<00:00, 406.83it/s, loss=0.131, v_num=8l_5]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████| 50/50 [00:00<00:00, 442.00it/s, loss=0.131, v_num=8l_5]\u001b[A\n",
      "Epoch 9: 100%|█████████| 50/50 [00:00<00:00, 415.97it/s, loss=0.131, v_num=8l_5]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(net,trn_dl,vld_dl)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73b96041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "log_dir = '/Users/vinay/Projects/uncertainty-estimates/nbs/logs/temp/version_5/'\n",
    "met = pd.read_csv(log_dir+'metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d99bb3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_acc</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.860</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>0.8450</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.354978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.872</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>0.8555</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.221781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.886</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.230131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.904</td>\n",
       "      <td>4</td>\n",
       "      <td>199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>199</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.916</td>\n",
       "      <td>5</td>\n",
       "      <td>239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>239</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.204086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.926</td>\n",
       "      <td>6</td>\n",
       "      <td>279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>279</td>\n",
       "      <td>0.9160</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.932</td>\n",
       "      <td>7</td>\n",
       "      <td>319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>319</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.169787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.940</td>\n",
       "      <td>8</td>\n",
       "      <td>359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>359</td>\n",
       "      <td>0.9380</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.946</td>\n",
       "      <td>9</td>\n",
       "      <td>399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>399</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_acc  epoch  step  train_acc  train_loss\n",
       "0     0.848      0    39        NaN         NaN\n",
       "1       NaN      0    39     0.7755         NaN\n",
       "2       NaN      1    49        NaN    0.532628\n",
       "3     0.860      1    79        NaN         NaN\n",
       "4       NaN      1    79     0.8450         NaN\n",
       "5       NaN      2    99        NaN    0.354978\n",
       "6     0.872      2   119        NaN         NaN\n",
       "7       NaN      2   119     0.8555         NaN\n",
       "8       NaN      3   149        NaN    0.221781\n",
       "9     0.886      3   159        NaN         NaN\n",
       "10      NaN      3   159     0.8755         NaN\n",
       "11      NaN      4   199        NaN    0.230131\n",
       "12    0.904      4   199        NaN         NaN\n",
       "13      NaN      4   199     0.8915         NaN\n",
       "14    0.916      5   239        NaN         NaN\n",
       "15      NaN      5   239     0.9050         NaN\n",
       "16      NaN      6   249        NaN    0.204086\n",
       "17    0.926      6   279        NaN         NaN\n",
       "18      NaN      6   279     0.9160         NaN\n",
       "19      NaN      7   299        NaN    0.181037\n",
       "20    0.932      7   319        NaN         NaN\n",
       "21      NaN      7   319     0.9310         NaN\n",
       "22      NaN      8   349        NaN    0.169787\n",
       "23    0.940      8   359        NaN         NaN\n",
       "24      NaN      8   359     0.9380         NaN\n",
       "25      NaN      9   399        NaN    0.098450\n",
       "26    0.946      9   399        NaN         NaN\n",
       "27      NaN      9   399     0.9510         NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d085d3",
   "metadata": {},
   "source": [
    "## wandb sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f199966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8dlplixp\n",
      "Sweep URL: https://wandb.ai/vin136/uncategorized/sweeps/8dlplixp\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_config = {\n",
    "  \"name\" : \"my-sweep\",\n",
    "  \"method\" : \"random\",\n",
    "  \"parameters\" : {\n",
    "    \"epochs\" : {\n",
    "      \"values\" : [10, 20, 50]\n",
    "    },\n",
    "    \"learning_rate\" :{\n",
    "      \"min\": 0.0001,\n",
    "      \"max\": 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a26b7d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'dmodel', 'optimizer/lr': 0.0002}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aae10c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vinay/Projects/uncertainty-estimates'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(os.getcwd().split('/')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f19c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import as_tensor,Tensor,ByteTensor,LongTensor,FloatTensor,HalfTensor,DoubleTensor\n",
    "from types import GeneratorType\n",
    "\n",
    "def is_listy(x):\n",
    "    \"`isinstance(x, (tuple,list,slice,GeneratorType))`\"\n",
    "    return isinstance(x, (tuple,list,slice,GeneratorType))\n",
    "\n",
    "\n",
    "\n",
    "def apply(func, x, *args, **kwargs):\n",
    "    \"Apply `func` recursively to `x`, passing on args\"\n",
    "    if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n",
    "    if isinstance(x,dict):  return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n",
    "    res = func(x, *args, **kwargs)\n",
    "    return res \n",
    "\n",
    "def to_device(b, device=None, non_blocking=False):\n",
    "    \"Recursively put `b` on `device`.\"\n",
    "    #if defaults.use_cuda==False: device='cpu'\n",
    "    #elif device is None: device=default_device()\n",
    "    def _inner(o):\n",
    "        if isinstance(o,Tensor): return o.to(device, non_blocking=non_blocking)\n",
    "#         if hasattr(o, \"to_device\"): return o.to_device(device)\n",
    "        return o\n",
    "    return apply(_inner, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405269b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
